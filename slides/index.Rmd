---
title: Assessing Amazon Turker and automated machine forecasts in the Hybrid Forecasting Competition
author: "Predictive Heuristics </br>Andreas Beger and Michael D. Ward"
date: "`r format(Sys.Date(), '%e %B %Y')` </br>Asia POLMETH 2019, Kyoto, Japan"
output: 
  xaringan::moon_reader:
    css: ["default-fonts", "default", "custom_theme.css"]
    chakra: libs/remark-latest.min.js
    nature:
      beforeInit: "macros.js"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(icon)
```

<!-- https://apreshill.github.io/data-vis-labs-2018/slides/06-slides_xaringan.html#13 -->

class: center, middle

.left.large[I'm going to show results of an exploratory analysis of the relative accuracy of volunteer, Amazon Mechanical Turker, and automated machine forecasts for a broad set of questions (IFPs)<sup>1</sup> in the HFC<sup>2</sup> during the first trial period (RCT-A<sup>3</sup>) that took place this year.]

.left.footnote[<sup>1</sup> Individual Forecasting Problem</br>
<sup>2 </sup> Hybrid Forecasting Competition </br>
<sup>3 </sup> Randomized controlled trial A]


---
class: middle, center

# Disclaimer

.left[This research is based upon work supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via 2017-17071900005. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.]


---
class: center, middle, inverse

# Hybrid Forecasting Competition

.pull-left[![](img/hfc-logo.png)]

.pull-right[![](img/sage-logo.png)]

---
class: middle 

# Project goal

.middle[
>The HFC program is developing and testing hybrid geopolitical forecasting systems. These systems **integrate human and machine forecasting components** to create maximally accurate, flexible, and scalable forecasting capabilities.
]

.right.small[From https://www.hybridforecasting.com]

The idea is to overcome some of the respective weaknesses of human and machine-generated forecasts by combining them in some to be determined fashion.

???

Human-generated forecasts are flexible, but subject to cognitive biases and scalability issues.

Machine-generated forecasts are reliable and scalable, but ill-suited for idiosyncratic or emerging issues.


---
background-image: url('img/screenshot-splash.png')
background-size: contain


---
background-image: url('img/screenshot-question.png')
background-size: contain


---

# Scoring

Multinomial Brier score:

$$mBS = \sum_{i=1}^R(f_i - o_i)^2$$

- $R$ is the number of answer options, $f$ the vector of weights summing to 1, and $o$ a 0/1 vector marking the correct option
- ranges from 0 (good) to 2 (bad)

--

Ordered Brier score: 

1. Split the ordinal categories (A-B-C-D) into cumulative binary pairs, aggregating the forecast probabilities for each grouping of categories (A-BCD; AB-CD; ABC-D). 
2. Calculate the multinomial Brier score for each of the binary categories.
3. Average across the binary category scores to obtain the final Brier score. 

- also ranges from 0 to 2
- "near misses" are penalized less than far misses


---
class: center, middle, inverse

# The hybrid part:

# Some users could see time series charts and/or machine forecasts for some IFPs


---

# How this works

- An automated system is collecting and updating data from several data sources, and matching them up, if possible, to questions

If data for a particular question is available:

- Chart the data
- An automated system generates a machine forecast


---
class: middle

### How much crude oil will Iraq produce in May 2018?

.center[
![:scale 80%](img/opec-iraq-with-chart.png)
]

- Less than 4,280
- Between 4,280 and 4,384, inclusive
- More than 4,384 but less than 4,473
- Between 4,473 and 4,576, inclusive
- More than 4,576

---
class: middle

### How much crude oil will Iraq produce in May 2018?

.center[
![:scale 80%](img/opec-iraq-with-forecast.png)
]

- Less than 4,280 *(38%)*
- Between 4,280 and 4,384, inclusive *(15%)*
- More than 4,384 but less than 4,473 *(13%)*
- Between 4,473 and 4,576, inclusive *(13%)*
- More than 4,576 *(21%)*

???

The model is ARIMA(0, 1, 1)

---

# basil-ts

Automated time series forecaster microservice

Implemented in R + Python Flask + RESTful API

Sketch of the internals: 

1. Parse incoming question and data
2. Produce a time series forecast using an automated ARIMA fitter<sup>1</sup>
3. Convert the time series forecast to answer option probabilities

Most of the complexity is related to automating the question/task parsing and handling edge cases.

.footnote[<sup>1</sup> [Hyndman & Khandakar 2008](http://www.jstatsoft.org/article/view/v027i03); R [forecast](https://cran.r-project.org/package=forecast) package]

???

Examples of parsing that needs to be done:

- how far ahead to forecast, which requires parsing the data and question time periods
- whether the data are count and should be constrained
- partial data handling
- data aggregation for fixed period questions

---

# Questions that were of primary concern to us

How well are the machine forecasts doing?

Are there particular question groups where performance is good or lacking? Basically, where do we need to focus improvements?

---
class: middle, center, inverse

# Let's start looking at data and results

---

# Data

RCT-A, the first trial period, lasted from 7 March to 7 September 2018

- Use subset of forecasts from 2 May (turkers enter) to 1 August 2018 (change in tracking)

~49,000 forecasts

156 IFPs; 46 with machine forecasts

971 unique users

Forecasters: 

- Volunteers who joined the platform 
- Amazon Mechanical Turkers
- Machine (basil-ts)

---

# Summary of some findings in the paper

--

Volunteers are better forecasts than Amazon Turkers

Machines are somewhere in the middle

--

Human forecasters who saw the machine forecasts did poorly; volunteer forecasts who saw only charts had overall the best performance

--

It's not clear why though, e.g.:

--

- they outperformed on questions that did not have data (charts and model)

--

- they outperformed volunteer forecasters who saw the machine forecasts even when the machine forecasts had good accuracy

--

Relative performance: where did the machine forecasts do better than human forecasters?

---

---

# Performance by forecaster group

|Forecaster | avg Brier|     N|
|:----------|---------:|-----:|
|Machine    |     0.402|  1975|
|Turker     |     0.433| 39140|
|Volunteer  |     0.322|  7816|

---

# Relative forecaster performance

Group questions by data source, N=14 groups

Linear model to compare average Brier scores of volunteer, turker, and machine forecasts (reference category)

$$\textrm{Brier}_{ij} = \beta_{i}\textrm{DataSource}_i + \beta'_{ij}(\textrm{DataSource}_i\times\textrm{Forecaster}_j)$$

$\beta_i$ = average Brier score for machine forecasts for question group $i$

$\beta'_{ij}$ = average Brier score *relative to machine forecasts* for forecaster group $j$ in question group $i$

- Negative values $\rightarrow$ human forecasters did better
- Positive values $\rightarrow$ machine forecasts did better

---
class: center, middle

![](img/model-machine-by-data-source-buildup.png)

???

- explain the two panels
- x-axis left is average machine forecast performance, right is difference from left; both are Brier scores
- y-axis are question groups, number is number of questions
- sorted so that at top are better human, at bottom are better machine

---
class: center, middle

![](img/model-machine-by-data-source.png)

???

- overall easiest, which can't see directly were oil prices, stocks
- overall hardest were ACLED, OECD interest rates, and Boko Haram
- humans generally tended to do better on questions with monthly period (food prices, oil production)
- machines more clearly did better on questions that require data aggregation (ACLED, Nigeria, hacking); earthquakes aggregates are readily available
- ACLED 2op versus 5op: maybe partly 0's, partly truncation issue

---

# ACLED 5-option

Overall hard questions; machine outperformed human forecasts

> How many battle deaths will ACLED record in Central African Republic in August 2018?

![](img/chart-2117.png)


---

# OECD interest rates

Overall hard questions; machine has middling performance

> What will be the long-term interest rate for Hungary (HUN) in July 2018?

![](img/chart-1793.png)


---

# ACLED binary question

Relatively easy questions, but machines underperformed a lot

> Will ACLED record any riot/protest events in Gambia in July 2018?

![](img/chart-1136.png)

---

# Conclusions

Machine forecasts did well on count questions that require data aggregation 

- ACLED
- Privacyrights hacking
- Nigeria security tracker (Boko Haram)

--

Some of the overall hardest questions, and where to some extent human forecasters did better, are economic/financial monthly series 

- OECD interest rates
- FAO food price indices
- exchange rates
- oil production

---

# Issues we did not address

--

What about selection issues; are volunteer forecasters able to self-select into questions they will do well on?

--

Do the chart volunteers do better on questions requiring data aggregation; generally there are some inconsistencies on why/how the chart volunteers did so well.


---
class: center, middle, inverse

# Thank you!

Register to forecast at https://sage-platform.isi.edu/

.left[
`r icon::fa("envelope")`: [adbeger@gmail.com](mailto:adbeger@gmail.com)

`r icon::fa("github-square")`: https://github.com/andybega/asia-polmeth-2019

`r icon::fa("file")`: [link to paper (on github under docs/pdf/)](pdf/BegerWard_HFC_AsiaPolmeth2019.pdf)
]



